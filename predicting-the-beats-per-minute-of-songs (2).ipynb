{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T02:17:45.170107Z","iopub.execute_input":"2025-09-27T02:17:45.170803Z","iopub.status.idle":"2025-09-27T02:17:45.177472Z","shell.execute_reply.started":"2025-09-27T02:17:45.170774Z","shell.execute_reply":"2025-09-27T02:17:45.176583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os, gc, warnings, random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom scipy.optimize import nnls\n\n\nSEED      = 42\nrandom.seed(SEED); np.random.seed(SEED)\n\nN_SPLITS  = 5          \nLRATE     = 0.03\nEARLY     = 300\nN_EST     = 20000\n\nTARGET    = \"BeatsPerMinute\"\nIDCOL     = \"id\"       \nINPUT_DIR = \"/kaggle/input/playground-series-s5e9\"\n\nWINSOR    = True      \nWQ        = 0.005      \n\n\n# 1) Load\n# ---------------------------\ntrain = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\ntest  = pd.read_csv(f\"{INPUT_DIR}/test.csv\")\nsub   = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\")\n\nassert TARGET in train.columns, f\"Target '{TARGET}' not found.\"\nif IDCOL not in train.columns:\n    if \"ID\" in train.columns: IDCOL = \"ID\"\n    else: raise ValueError(\"ID column not found (expected 'id' or 'ID').\")\n\ny = train[TARGET].astype(np.float32).values\nfeature_cols = [c for c in train.columns if c not in [TARGET, IDCOL]]\n\nX_raw     = train[feature_cols].copy()\nXtest_raw = test[feature_cols].copy()\n\ncat_cols = X_raw.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\nnum_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\nprint(f\"Detected -> numeric: {len(num_cols)} | categorical: {len(cat_cols)}\")\n\n\n# 2) Winsorize numeric \n# ---------------------------\nif WINSOR and len(num_cols) > 0:\n    ql = X_raw[num_cols].quantile(WQ)\n    qh = X_raw[num_cols].quantile(1 - WQ)\n    X_raw[num_cols]     = X_raw[num_cols].clip(ql, qh, axis=1)\n    Xtest_raw[num_cols] = Xtest_raw[num_cols].clip(ql, qh, axis=1)\n\n\n# 3) Frequency + OOF Target Encoding \n# ---------------------------\nGLOBAL_MEAN = float(y.mean())\n\ndef add_cat_encodings(train_df, test_df, y_vec, cat_cols, n_splits=N_SPLITS, seed=SEED):\n    if not cat_cols:\n        return pd.DataFrame(index=train_df.index), pd.DataFrame(index=test_df.index)\n\n    # Frequency (count) encoding\n    freq_tr = pd.DataFrame(index=train_df.index)\n    freq_te = pd.DataFrame(index=test_df.index)\n    for c in cat_cols:\n        vc = train_df[c].value_counts(dropna=False)\n        freq_tr[f\"{c}_freq\"] = train_df[c].map(vc).astype(\"float32\")\n        freq_te[f\"{c}_freq\"] = test_df[c].map(vc).fillna(0).astype(\"float32\")\n\n    # OOF target mean encoding\n    te_tr = pd.DataFrame(index=train_df.index)\n    te_te = pd.DataFrame(index=test_df.index)\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    for c in cat_cols:\n        oof_col = np.zeros(len(train_df), dtype=np.float32)\n        for tr_idx, va_idx in kf.split(train_df):\n            m = (\n                train_df.iloc[tr_idx][c]\n                .to_frame()\n                .join(pd.Series(y_vec[tr_idx], index=train_df.index[tr_idx], name=\"y\"))\n                .groupby(c)[\"y\"].mean()\n            )\n            oof_col[va_idx] = train_df.iloc[va_idx][c].map(m).fillna(GLOBAL_MEAN).astype(\"float32\").values\n        te_map_full = (\n            train_df[c]\n            .to_frame()\n            .join(pd.Series(y_vec, index=train_df.index, name=\"y\"))\n            .groupby(c)[\"y\"].mean()\n        )\n        te_tr[f\"{c}_te\"] = oof_col\n        te_te[f\"{c}_te\"] = test_df[c].map(te_map_full).fillna(GLOBAL_MEAN).astype(\"float32\")\n\n    enc_tr = pd.concat([freq_tr, te_tr], axis=1)\n    enc_te = pd.concat([freq_te, te_te], axis=1)\n    return enc_tr, enc_te\n\nenc_tr, enc_te = add_cat_encodings(X_raw, Xtest_raw, y, cat_cols, n_splits=N_SPLITS, seed=SEED)\n\n\n# 4) Build final design matrix \n# ---------------------------\nfrom sklearn.impute import SimpleImputer\nif len(num_cols):\n    imp = SimpleImputer(strategy=\"median\")\n    X_num      = imp.fit_transform(X_raw[num_cols]).astype(np.float32)\n    Xtest_num  = imp.transform(Xtest_raw[num_cols]).astype(np.float32)\nelse:\n    X_num      = np.zeros((len(X_raw), 0), dtype=np.float32)\n    Xtest_num  = np.zeros((len(Xtest_raw), 0), dtype=np.float32)\n\nX = np.concatenate([X_num, enc_tr.values.astype(np.float32)], axis=1)\nX_test = np.concatenate([Xtest_num, enc_te.values.astype(np.float32)], axis=1)\nprint(\"Final design matrix:\", X.shape, X_test.shape)\n\ndef rmse(a,b): return mean_squared_error(a,b,squared=False)\n\n\n# 5) StratifiedKFold on binned target\n# ---------------------------\nbins = pd.qcut(y, q=min(10, max(2, len(np.unique(y))//2)),\n               labels=False, duplicates=\"drop\")\nskf  = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n\n# 6) LightGBM (seed-bag), memory-lean\n# ---------------------------\nSEEDS = [42, 2025]   # add a 3rd (e.g., 7) for final if time permits\nlgb_oof  = np.zeros(len(y), dtype=np.float32)\nlgb_test = np.zeros(X_test.shape[0], dtype=np.float32)\n\nfor s in SEEDS:\n    tmp_oof = np.zeros(len(y), dtype=np.float32)\n    skf_s = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=s)\n    for fold, (tr, va) in enumerate(skf_s.split(X, bins), 1):\n        X_tr, X_va, y_tr, y_va = X[tr], X[va], y[tr], y[va]\n        lgbm = LGBMRegressor(\n            n_estimators=N_EST, learning_rate=LRATE,\n            num_leaves=48, max_depth=-1,\n            subsample=0.8, colsample_bytree=0.75,\n            min_child_samples=60, reg_lambda=1.5, reg_alpha=0.1,\n            metric=\"rmse\", random_state=s + fold\n        )\n        lgbm.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            callbacks=[lgb.early_stopping(EARLY), lgb.log_evaluation(0)]\n        )\n        p = lgbm.predict(X_va).astype(np.float32)\n        tmp_oof[va] = p\n        lgb_test += lgbm.predict(X_test).astype(np.float32) / (N_SPLITS * len(SEEDS))\n        del X_tr, X_va, y_tr, y_va, lgbm, p; gc.collect()\n    lgb_oof += tmp_oof / len(SEEDS)\n\nlgb_cv = rmse(y, lgb_oof)\nprint(f\"LGBM (bagged) CV RMSE: {lgb_cv:.5f}\")\n\n\n# 7) XGBoost (hist), regularized\n# ---------------------------\nxgb_oof  = np.zeros(len(y), dtype=np.float32)\nxgb_test = np.zeros(X_test.shape[0], dtype=np.float32)\n\nfor fold, (tr, va) in enumerate(skf.split(X, bins), 1):\n    X_tr, X_va, y_tr, y_va = X[tr], X[va], y[tr], y[va]\n    xgbm = xgb.XGBRegressor(\n        n_estimators=N_EST, learning_rate=LRATE,\n        max_depth=6, min_child_weight=8.0,\n        subsample=0.8, colsample_bytree=0.7,\n        reg_lambda=3.0, reg_alpha=0.0, gamma=0.15,\n        max_bin=256, tree_method=\"hist\", predictor=\"auto\",\n        random_state=SEED + fold, nthread=-1\n    )\n    xgbm.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=EARLY,\n        verbose=False\n    )\n    p = xgbm.predict(X_va).astype(np.float32)\n    xgb_oof[va] = p\n    xgb_test += xgbm.predict(X_test).astype(np.float32) / N_SPLITS\n    del X_tr, X_va, y_tr, y_va, xgbm, p; gc.collect()\n\nxgb_cv = rmse(y, xgb_oof)\nprint(f\"XGB  CV RMSE: {xgb_cv:.5f}\")\n\n\n# 8) NNLS blend + calibration\n# ---------------------------\noof_mat  = np.vstack([lgb_oof, xgb_oof]).T.astype(np.float64)\ntest_mat = np.vstack([lgb_test, xgb_test]).T.astype(np.float64)\nw, _     = nnls(oof_mat, y.astype(np.float64))\nw        = w / (w.sum() if w.sum()!=0 else 1.0)\nens_oof  = oof_mat @ w\nens_test = test_mat @ w\nprint(\"NNLS weights [LGBM, XGB]:\", np.round(w,4))\nprint(\"Ensemble OOF RMSE:\", rmse(y, ens_oof))\n\n# linear mean-variance calibration on OOF, apply to test\nmu_y, mu_o = float(y.mean()), float(ens_oof.mean())\nvar_o = float(ens_oof.var()) + 1e-12\ncov_  = float(((ens_oof - mu_o)*(y - mu_y)).mean())\nb = cov_ / var_o; a = mu_y - b*mu_o\nens_oof_cal  = a + b*ens_oof\nens_test_cal = a + b*ens_test\nuse_cal = rmse(y, ens_oof_cal) <= rmse(y, ens_oof)\nfinal_pred = ens_test_cal if use_cal else ens_test\nprint(\"Calibration used:\", use_cal)\n\n\n# 9) Exact-duplicate row signature override\n# ---------------------------\ndef row_sig(df):\n    tmp = df.copy()\n    for c in tmp.columns:\n        tmp[c] = tmp[c].astype(str).fillna(\"NA\")\n    return pd.util.hash_pandas_object(tmp, index=False)\n\nsig_train = row_sig(train[feature_cols])\nsig_test  = row_sig(test[feature_cols])\n\nsig_to_mean = pd.DataFrame({\"sig\": sig_train, \"y\": y}).groupby(\"sig\")[\"y\"].mean()\noverride_idx = sig_test.isin(sig_to_mean.index)\nif override_idx.any():\n    overrides = sig_test[override_idx].map(sig_to_mean).values.astype(np.float32)\n    final_pred[override_idx.values] = overrides\n    print(f\"Signature overrides applied to {override_idx.sum()} test rows.\")\n\n\n# 10) Clip + tiny debias & save files\n# ---------------------------\ngmean = float(train[TARGET].mean())\nfinal_pred = np.clip(final_pred, 40, 220)\nfinal_pred = 0.98*final_pred + 0.02*gmean\n\n# Ensemble submission\nsub1 = sub.copy()\nsub1[\"BeatsPerMinute\"] = final_pred.astype(np.float32)\nsub1.to_csv(\"submission.csv\", index=False)\nprint(\"Wrote submission.csv (ensemble)\")\n\n# Single-model hedge (LGBM)\nsingle = np.clip(lgb_test, 40, 220).astype(np.float32)\nsub2 = sub.copy()\nsub2[\"BeatsPerMinute\"] = 0.98*single + 0.02*gmean\nsub2.to_csv(\"submission_lgbm.csv\", index=False)\nprint(\"Wrote submission_lgbm.csv (single)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:55:17.908749Z","iopub.execute_input":"2025-09-30T15:55:17.909081Z","iopub.status.idle":"2025-09-30T15:57:36.666857Z","shell.execute_reply.started":"2025-09-30T15:55:17.909057Z","shell.execute_reply":"2025-09-30T15:57:36.665849Z"}},"outputs":[],"execution_count":null}]}